	Sentiment	Analysis	by	Text	Classification
Download	“amazon_review_300.csv”.	This	dataset	has	three	columns:	label,	title,	and	text.	In	
this	assignment,	we	build	classifiers using	label	and	text	columns. The	classifiers	are	used	to
detect	review	sentiment:	1	or	2.
1. Experiment	1:	Naïve	Base	classifier	with	cross	validation
Write	a	block	of	code	to	create a MultinomialNB classifier	with	6-fold cross-validation using	
tf-idf	matrix	with	stop	words	removed.	Report	macro	precision/recall/f1	score	for	each	fold.	
2. Experiment	2:	Tune	parameters	using	grid search
Write	a	block	of	code	to	tune	the	classifier	you	created	in	(2) using	grid	search.	The	grid	
search	is	to	find	best	values	for	the	following	parameters:	
• stop_words:	[None,"english"]
• min_df:	[1,2,3,5]
• alpha:	[0.5,1.0,1.5,2.0]
Compare	the	performance	with	the	classifier	in	(2)	and	write your	conclusion.
3. Experiment	3:	How	many	samples	are	enough? Show	the	impact	of	sample	size on	
classifier	performance
Download	“amazon_review_large.csv”	which	contains	20,000	reviews.	Starting	with	400	
samples,	in	each	round	you	build	a	classifier	with	400	more	samples.	i.e.	in	round	1,	you use	
samples	from	0:400,	and	in round 2,	you	use	samples	from	0:800,	…, until	you	use	all	
samples.	In	each	round,	do the	following:
a. create	tf-idf	matrix	using	TfidfVectorizer with	stop	words	removed
b. train	the	classifier	using	linearSVC model	with	10-fold	cross	validation
c. train	another	classifier	using	multinomialNB with	10-fold	cross	validation
d. For	each	classifier,	collect	the average	macro	f1 score	over	10	folds
Draw	a	line	chart	show	the	relationship	between	sample	size	and	average	macro	f1	score.	
Write	your	analysis	on	the	following:
• How	sample	size	affects	each	classifier’s	performance?	
• How	many	samples	do	you	think	would	be	needed	for	each	model	for	good	
performance?
• How	is	performance	of	SVM	classifier	compared	with	Naïve	Bayes	classifier?
e. (Bonus):	Analyze	model	stability	of	these	two	models	as	the	sample	size increases.	You	
can	use	f-score	variance	(or	std) over	folds	to	indicate	model	stability.	The	smaller	the	
variance,	the	more	stable	a model.


Code 

from sklearn.naive_bayes import MultinomialNB
from IPython.core.interactiveshell import InteractiveShell
InteractiveShell.ast_node_interactivity = "all"
from sklearn.feature_extraction.text import TfidfVectorizer
import csv
from sklearn.model_selection import train_test_split
import pandas as pd
# import method to calculate metrics
from sklearn.metrics import precision_recall_fscore_support
from sklearn.metrics import classification_report
from sklearn.model_selection import cross_validate
from sklearn.metrics import precision_recall_fscore_support
from sklearn.naive_bayes import MultinomialNB
from sklearn.pipeline import Pipeline
import numpy as np
from sklearn.model_selection import GridSearchCV
from sklearn import svm
from sklearn.svm import LinearSVC
import matplotlib.pyplot as plt






# initialize the TfidfVectorizer 
def Multinomial():
    tfidf_vect = TfidfVectorizer() 
    target=[]
    text=[]


    df = pd.read_csv('/Users/yash/Downloads/amazon_review_300.csv',delimiter=',',names=["Label","title", "review"])
    df.head()

    text = list(df['review'])
    target = list(df['Label'])

#text,nr,target=zip(*data)
#target=list(text)
#text=list(target) 
# print(text[0])
# print(target[0])
    tfidf_vect = TfidfVectorizer()
    tfidf_vect = TfidfVectorizer(stop_words="english") 

# generate tfidf matrix
    dtm= tfidf_vect.fit_transform(text)
# import method for split train/test data set
    print('type of dtm:', type(dtm))
    print('shape of tfidf matrix:', dtm.shape)

# split dataset into train (70%) and test sets (30%)
    X_train, X_test, y_train, y_test = train_test_split(\
                dtm, target, test_size=0.3, random_state=0)

# train a multinomial naive Bayes model using the testing data
    clf = MultinomialNB().fit(X_train, y_train)

# predict the news group for the test dataset
    predicted=clf.predict(X_test)

# get the list of unique labels
    labels=sorted(list(set(target)))

# calculate performance metrics. 
# Support is the number of occurrences of each label

    precision, recall, fscore, support=\
     precision_recall_fscore_support(\
     y_test, predicted, labels=labels)

    print(labels)
    print(precision)
    print(recall)
    print(fscore)
    print(support)
    metrics= ['precision_macro','recall_macro','f1_macro']
    clf=MultinomialNB(alpha=0.8)
    cv=cross_validate(clf,dtm,target,scoring=metrics,cv=6)
    print("Test data set average precision:")
    print(cv['test_precision_macro'])
    print("\nTest data set average recall:")
    print(cv['test_recall_macro'])
    print("\nTest data set average fscore:")
    print(cv['test_f1_macro'])


    print(classification_report(y_test, predicted, labels=labels))


def Grid():
    tfidf_vect = TfidfVectorizer() 
    target=[]
    text=[]
   

    df = pd.read_csv('/Users/yash/Downloads/amazon_review_300.csv',delimiter=',',names=["Label","title", "review"])
    df.head()

    text = list(df['review'])
    target = list(df['Label'])

#text,nr,target=zip(*data)
#target=list(text)
#text=list(target) 
# print(text[0])
# print(target[0])
    tfidf_vect = TfidfVectorizer()
    tfidf_vect = TfidfVectorizer(stop_words="english") 

# generate tfidf matrix
    dtm= tfidf_vect.fit_transform(text)
# import method for split train/test data set
    print('type of dtm:', type(dtm))
    print('shape of tfidf matrix:', dtm.shape)

# split dataset into train (70%) and test sets (30%)
    X_train, X_test, y_train, y_test = train_test_split(\
                dtm, target, test_size=0.3, random_state=0)

# train a multinomial naive Bayes model using the testing data
    clf = MultinomialNB().fit(X_train, y_train)

# predict the news group for the test dataset
    predicted=clf.predict(X_test)

# get the list of unique labels
    labels=sorted(list(set(target)))

# calculate performance metrics. 
# Support is the number of occurrences of each label

    precision, recall, fscore, support=\
     precision_recall_fscore_support(\
     y_test, predicted, labels=labels)

    print(classification_report(y_test, predicted, labels=labels))



    text_clf = Pipeline([('tfidf', TfidfVectorizer()),
                     ('clf', MultinomialNB())
                   ])


    parameters = {'tfidf__min_df':[1,2,3,5],
              'tfidf__stop_words':[None,"english"],
              'clf__alpha': [0.5,1.0,1.5,2.0],
                                          }

# the metric used to select the best parameters
    metric =  "f1_macro"

# GridSearch also uses cross validation
    gs_clf = GridSearchCV\
    (text_clf, param_grid=parameters, scoring=metric, cv=6)

# due to data volume and large parameter combinations
# it may take long time to search for optimal parameter combination
# you can use a subset of data to test
    gs_clf = gs_clf.fit(text, target)

    for param_name in gs_clf.best_params_:
        print(param_name,": ",gs_clf.best_params_[param_name])

    print("best f1 score:", gs_clf.best_score_)




def NB(text,target):
    
    text_clf = Pipeline([('tfidf', TfidfVectorizer(stop_words=None)),('clf1', MultinomialNB()),])
    cv = cross_validate(text_clf, text, target, scoring=metrics, cv=10)
    result1 = cv['test_f1_macro']
    return result1

def SVC(text,target):
    text_clf = Pipeline([('tfidf', TfidfVectorizer(stop_words=None)),('clf1', svm.LinearSVC()),])
    cv = cross_validate(text_clf, text, target, scoring=metrics, cv=10)

    
    return2= cv['test_f1_macro']
    return return2


if __name__ == '__main__':
    Multinomial()
    Grid()
    Classifier1=[]
    Classifier2=[]
    i=400
    a=[]
    b=[]
    target=[]
    text=[]
    metrics = ["f1_macro"]
    
    df = pd.read_csv('/Users/yash/Downloads/amazon_review_large.csv',delimiter=',',names=["Label", "review"])
    df.head()

    text = list(df['review'])
    target = list(df['Label'])
   
    while i<=20000:
        a = NB(text[0:i],target[0:i])
        a=np.mean(a)
        Classifier1.append(a)
        
        b = SVC(text[0:i],target[0:i])
        b =np.mean(b)
        Classifier2.append(b)
        i=i+400
        
    print('result of classifier 1', Classifier1)
    print('result of classifier 2', Classifier2)
    plt.plot(Classifier1)
    plt.plot(Classifier2)
    plt.legend(loc='center right')
    plt.show()
    
